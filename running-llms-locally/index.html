<!doctype html><html lang=en-us><head><title>Running LLMs locally // A fractal spectrum of tales</title>
<meta charset=utf-8><meta name=generator content="Hugo 0.129.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Marcello Seri"><meta name=description content="Free thoughts of a geeky mathematician"><meta http-equiv=Permissions-Policy content="interest-cohort=()"><link rel=stylesheet href=https://www.mseri.me/css/main.min.fb6dd6e5b8f0b7c4a9008884f1a95d7a84ef20fb9c190bcbbd87b6ff85f8fb9b.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Running LLMs locally"><meta name=twitter:description content="Large Language Models (LLMs) are powerful tools for generating human-like text responses. You might be familiar with them through services like ChatGPT, Anthropic Claude, Google Gemini, and Perplexity AI, Nowadays people are using them for editing purposes, writing, brainstorming, and even for generating code snippets. When used responsibly and critically as a tool to assist human creativity, they can be very helpful.
Recently, I spent some time playing with these models and I found them fascinating."><meta property="og:url" content="https://www.mseri.me/running-llms-locally/"><meta property="og:site_name" content="A fractal spectrum of tales"><meta property="og:title" content="Running LLMs locally"><meta property="og:description" content="Large Language Models (LLMs) are powerful tools for generating human-like text responses. You might be familiar with them through services like ChatGPT, Anthropic Claude, Google Gemini, and Perplexity AI, Nowadays people are using them for editing purposes, writing, brainstorming, and even for generating code snippets. When used responsibly and critically as a tool to assist human creativity, they can be very helpful.
Recently, I spent some time playing with these models and I found them fascinating."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-24T18:37:42+02:00"><meta property="article:modified_time" content="2024-07-24T18:37:42+02:00"><meta property="article:tag" content="Tutorial"><meta property="article:tag" content="Llms"></head><body><header class=app-header><a href=https://www.mseri.me/><img class=app-header-avatar src=/images/gauss_logo.png alt="Marcello Seri"></a><h1>A fractal spectrum of tales</h1><p>Free thoughts of a geeky mathematician</p><div class=app-header-social><a target=_blank href=https://github.com/mseri rel="noreferrer noopener me"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><title>github</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a>
<a target=_blank href=https://mathstodon.xyz/@mseri rel="noreferrer noopener me"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-mastodon"><title>mastodon</title><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.648 15.254C16.832 17.017 12 16.88 12 16.88a18.262 18.262.0 01-3.288-.256c1.127 1.985 4.12 2.81 8.982 2.475-1.945 2.013-13.598 5.257-13.668-7.636L4 10.309c0-3.036.023-4.115 1.352-5.633C7.023 2.766 12 3.01 12 3.01s4.977-.243 6.648 1.667C19.977 6.195 20 7.274 20 10.31s-.456 4.074-1.352 4.944z"/><path d="M12 11.204V8.278C12 7.02 11.105 6 10 6S8 7.02 8 8.278V13m4-4.722C12 7.02 12.895 6 14 6s2 1.02 2 2.278V13"/></svg></a>
<a target=_blank href=https://twitter.com/marcelloseri rel="noreferrer noopener me"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter"><title>twitter</title><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a>
<a target=_blank href=https://academic.mseri.me rel="noreferrer noopener me"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-award"><title>award</title><circle cx="12" cy="8" r="7"/><polyline points="8.21 13.89 7 23 12 20 17 23 15.79 13.88"/></svg></a></div><div style=margin-top:30px><h3 style=margin-bottom:10px>My podcasts</h4><a href=https://podcasters.spotify.com/pod/show/not-just-numbers><img src=/images/njn.jpg alt="Cover of the pocast it's not just numbers" class=image width=105px>
</a>&nbsp;
<a href=https://linktr.ee/degreesoffreedom><img src=/images/dof.jpg alt="Cover of the podcast degrees of freedom" class=image width=105px></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>Running LLMs locally</h1><div class=post-meta><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
Jul 24, 2024</div><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
6 min read</div><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<a class=tag href=https://www.mseri.me/tags/tutorial/>Tutorial</a><a class=tag href=https://www.mseri.me/tags/llms/>Llms</a></div></div></header><div class=post-content><p>Large Language Models (LLMs) are powerful tools for generating human-like text responses. You might be familiar with them through services like <a href=https://chat.openai.com/>ChatGPT</a>, <a href=https://claude.ai/>Anthropic Claude</a>, <a href=https://gemini.google.com/>Google Gemini</a>, and <a href=https://www.perplexity.ai/>Perplexity AI</a>,
Nowadays people are using them for editing purposes, writing, brainstorming, and even for generating code snippets.
When used <strong>responsibly and critically</strong> as a tool to assist human creativity, they can be very helpful.</p><p>Recently, I spent some time playing with these models and I found them fascinating. However, due to privacy concerns and their high environmental costs, I don&rsquo;t feel comfortable using cloud-based services.
This post is an account of my experience with running LLMs locally on my machines.
This can be quite straightforward, and if you have 8-16GB of RAM and a decent GPU, you can run these models on your own computer without significant issues.</p><p>All my experiments were conducted on my M1 iMac and Mac Mini, and I found the experience to be quite smooth.</p><p>I am going to describe two ways to run LLMs locally: using pre-made graphical applications and using the command line interface (CLI).
While I find more comfortable the latter, it requires a bit more setup and it can be more error-prone at first.</p><p>Before we dive in, let me clarify that I am not going to cover the ethical implications of using these models. I am not an expert on the subject, and I am not going to pretend to be one.</p><p>Moreover, be aware that you shouldn&rsquo;t expect answers as sophisticated as those from famous cloud-based services. The models that run on average (or even good) consumer hardware need to be greatly compressed at the expense of the quality of the response, but they are good enough for many use cases.</p><h2 id=the-easiest-way-guis><a class=hanchor href=#the-easiest-way-guis>#</a>&nbsp;The easiest way: GUIs</h2><p>There are a few applications that allow you to run LLMs locally with a graphical interface. In my opinion, the nicest and easiest to use is the open source project <a href=https://www.nomic.ai/gpt4all>GPT4All</a>.</p><p><video src=https://github.com/nomic-ai/gpt4all/assets/70534565/513a0f15-4964-4109-89e4-4f9a9011f311 controls muted style=max-height:640px;min-height:100px;width:100%></video></p><p>Just download the installer, run it, and you are kind of ready to go.</p><p>The project itself is <a href=https://docs.gpt4all.io/gpt4all_desktop/quickstart.html>well documented</a> but it can be confusing if you try to follow the steps in the video above: the first time you run the software, there is not yet any model downloaded, so you will need to download one first.
To do this, click on the <code>Download</code> button in the <code>Models</code> tab, and then select the model you want to download. This will take some time, so be patient.</p><p>Depending on the RAM and GPU you have, you can choose between different models. This is rather confusing, and very poorly explained in my opinion. It took some trial and error to find the right models for my system and one that was producing good enough answers for my use cases.</p><p>Selecting an appropriate model can be confusing due to limited documentation. Through trial and error, I found the following models worked well:</p><ul><li>For systems with 16GB RAM (e.g., my iMac):<ul><li><code>Nous Hermes 2 Mistral DPO</code></li><li><code>Llama 3 Instruct</code></li></ul></li><li>For systems with 8GB RAM (e.g., my Mac Mini):<ul><li><code>Phi-3 Mini Instruct</code></li></ul></li></ul><p>A benchmark and comparison of the main language models in the wild can be found on the <a href=https://chat.lmsys.org/?leaderboard>LMSYS Chatbot Arena Leaderboard</a>, this will give you an idea of the quality of the models you can run locally.</p><p>Once the model is downloaded and installed you can head to the <code>Chat</code> tab and start chatting with the model.
At some point you may notice that responses are cut out before they are finished. This is because the tool has a limit on the length of the response it can generate.
You can change this limit in the <code>Settings</code> tab, by increasing the number of tokens in the <code>Maximum Response Length</code> option.</p><p>If for some reasons you are not happy with <code>GPT4All</code>, you can try <a href=https://jan.ai/>Jan</a>.
It is also open source and, from what I understand, more performant than <code>GPT4All</code> with practically the same functionalities.</p><h2 id=the-geeky-way-command-line-interface><a class=hanchor href=#the-geeky-way-command-line-interface>#</a>&nbsp;The geeky way: command line interface</h2><p>This is the way I prefer to run LLMs locally.
It is more flexible and, in my opinion, more powerful than using a GUI.
And in my experiment turned out also to be more performant, about twice as fast as the GUI.</p><p>The best experience I had so far, was with the <code>llm</code> command from the <a href=https://github.com/simonw/llm>python llm library</a>.</p><p>Don&rsquo;t worry, you don&rsquo;t need to know how to use python to use it.
You just need to have python installed on your machine and a few steps to install the library.</p><p>To avoid messing up your system, I recommend to first install <a href=https://pipx.pypa.io/stable/>a tool called <code>pipx</code></a> that allows you to install python packages in isolated environments. You can follow the instructions of the link above or your favourite package manager to install it.</p><p>Once you have <code>pipx</code> installed, you can install the <code>llm</code> command by running the following command in your terminal:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pipx install llm
</span></span></code></pre></div><p>Before being able to chat with a model, you need to download one.
The easiest thing to do, is to install a plugin that enables support for all the <code>GPT4All</code> mofels.</p><p>You can do this by running the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm install llm-gpt4all
</span></span></code></pre></div><p>Then you can check the available models by running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm models
</span></span></code></pre></div><p>The model will be downloaded in the background the first time you try using it. For example, to chat with <code>Phi-3 Mini</code> model you can run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm chat -m Phi-3-mini-4k-instruct
</span></span></code></pre></div><p>Also in this case, if you notice that the responses are cut too short, try extending the maximum number of tokens. You can do this by running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm chat -m Phi-3-mini-4k-instruct -p max_length <span style=color:#ae81ff>4096</span>
</span></span></code></pre></div><p>Have fun!</p><p>Note: the <code>Phi3 mini</code> <a href=https://github.com/simonw/llm-gpt4all/issues/30>does not yet quite work with <code>llm</code></a>, although the output is often usable.
You can still try it <a href=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf#how-to-use-with-llamafile>following the instructions here</a>, at least, that is what I have done while waiting for the fix to land :)</p><p>The <code>llm</code> library is really flexible, have a look at its documentation to get an idea of the many ways you can interact with it.</p><p>The author of this tool has a nice blog full of examples of use, and is always on top of the novelties. For example you can already use it to try <a href=https://simonwillison.net/2024/Jul/23/llm-gguf/>the new Llama 3.1 model</a>.
Quoting its blog post, you can install the model by running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm install llm-gguf
</span></span><span style=display:flex><span>llm gguf download-model <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --alias llama-3.1-8b-instruct --alias l31i
</span></span></code></pre></div><p>and then chat with it as we did above, runinng:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>llm chat -m l31i
</span></span></code></pre></div><h2 id=the-harder-way-coding-your-own-interaction><a class=hanchor href=#the-harder-way-coding-your-own-interaction>#</a>&nbsp;The harder way: coding your own interaction</h2><p>If you are like me and you prefer to use the command line, you can use the <a href=https://huggingface.co/transformers/>Hugging Face Transformers</a> library to run LLMs locally.
While powerful, this approach requires more coding knowledge and may be overkill unless you need very specific functionality.</p><p>I have played a bit with this library and I found it to be very powerful and relatively easy to use, but so far I never needed the extra flexibility it provides.
It is so easy to use the <code>llm</code> command above that I don&rsquo;t see any reason to use this method unless you want to code your own interaction with the model.</p></div><div class=post-footer><script src=https://utteranc.es/client.js repo=mseri/mseri.github.io issue-term=pathname label=âœ¨ðŸ’¬âœ¨ theme=photon-dark crossorigin=anonymous async></script></div></article></main></body></html>